{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO47XEJTNG/SEvLnOIuyM1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlarsonhlarson/perceptron/blob/master/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o9TSKoeLBWN"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7MiZumKZS_"
      },
      "source": [
        "class MultiLayerPerceptron:\r\n",
        "\r\n",
        "  def __init__(self, learning_rate, iter_nums):\r\n",
        "    self.lr = learning_rate\r\n",
        "    self.n_iters = iter_nums\r\n",
        "    self. activation_func = self.sigmoid()\r\n",
        "    self.weights = None\r\n",
        "    self.bias = None\r\n",
        "  \r\n",
        "  def sigmoid(self, x):\r\n",
        "    return 1 / (1 + np.exp(-x))\r\n",
        "\r\n",
        "  def init_weigths(L_in, L_out, epsilon_init=0.12):\r\n",
        "    \"\"\"\r\n",
        "    Randomly initialize the weights of a layer in a neural network.\r\n",
        "    \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    L_in : int\r\n",
        "        Number of incomming connections.\r\n",
        "    \r\n",
        "    L_out : int\r\n",
        "        Number of outgoing connections. \r\n",
        "    \r\n",
        "    epsilon_init : float, optional\r\n",
        "        Range of values which the weight can take from a uniform \r\n",
        "        distribution.\r\n",
        "    \r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    W : array_like\r\n",
        "        The weight initialiatized to random values.  Note that W should\r\n",
        "        be set to a matrix of size(L_out, 1 + L_in) as\r\n",
        "        the first column of W handles the \"bias\" terms.\r\n",
        "    \"\"\"\r\n",
        "    W = np.zeros((L_out, 1 + L_in))\r\n",
        "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\r\n",
        "    return W\r\n",
        "\r\n",
        "  def softmax(self, x):\r\n",
        "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\r\n",
        "    shiftx = x - np.max(x)\r\n",
        "    exps = np.exp(shiftx)\r\n",
        "    return exps / np.sum(exps, axis=0)\r\n",
        "\r\n",
        "  def sigmoid_gradient(x):\r\n",
        "    sig = sigmoid(x)\r\n",
        "    return sig * (1-sig)\r\n",
        "  \r\n",
        "  def softmax_gradient(x):\r\n",
        "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\r\n",
        "    s = x.reshape(-1,1)\r\n",
        "    return np.diagflat(s) - np.dot(s, s.T)\r\n",
        "\r\n",
        "  def fit(self, X, y):\r\n",
        "    n_samples, n_features = X.shape\r\n",
        "\r\n",
        "    self.init_weigths(dim)\r\n",
        "    for _ in range(self.n_iters):\r\n",
        "\r\n",
        "  def predict(self, X):\r\n",
        "    linear_output = np.dot(X, self.weigths) + self.bias\r\n",
        "    y_predicted = self.activation_func(linear_output)\r\n",
        "    return y_predicted\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}